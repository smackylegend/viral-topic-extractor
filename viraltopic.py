import streamlit as st import requests import pandas as pd from datetime import datetime, timedelta, timezone # ----------------------------- # CONFIG # ----------------------------- API_KEY = "AIzaSyAUHpprPXVoeRc9R_0vc77PXZEjxRXOUwg" # âœ… better: Streamlit Secrets use karo YOUTUBE_SEARCH_URL = "https://www.googleapis.com/youtube/v3/search" YOUTUBE_VIDEO_URL = "https://www.googleapis.com/youtube/v3/videos" st.set_page_config(page_title="Viral Topics Tool", layout="wide") st.title("YouTube Viral Topics Tool (Last Days Viral by Views)") # ----------------------------- # INPUTS # ----------------------------- days = st.number_input("Last how many days? (default 3)", min_value=1, max_value=30, value=3) st.caption("Keywords: one per line (apni marzi se edit karo)") default_kw = """Affair Relationship Stories """ keywords_text = st.text_area("Keywords", value=default_kw, height=160, key="kw_editor") keywords = [k.strip() for k in keywords_text.split("\n") if k.strip()] exclude_shorts = st.checkbox("Exclude Shorts / very short videos?", value=True) # If exclude_shorts = True â†’ medium+ only (4 min+). If False â†’ any duration. duration_filter = "medium" if exclude_shorts else "any" max_per_keyword = st.selectbox("Results per keyword", [10, 25, 50], index=2) # ----------------------------- # HELPERS # ----------------------------- def yt_get(url, params): r = requests.get(url, params=params, timeout=30) if r.status_code != 200: return None, f"HTTP {r.status_code}: {r.text[:200]}" return r.json(), None def chunk(lst, n=50): for i in range(0, len(lst), n): yield lst[i:i+n] def parse_yt_time(ts): # example: 2026-02-07T10:20:30Z return datetime.fromisoformat(ts.replace("Z", "+00:00")) # ----------------------------- # RUN # ----------------------------- if st.button("Fetch Viral Videos"): if not API_KEY or API_KEY == "PASTE_YOUR_API_KEY_HERE": st.error("API_KEY set karo (aur behtar hai Secrets me).") st.stop() if not keywords: st.warning("Please add at least 1 keyword.") st.stop() start_date = (datetime.now(timezone.utc) - timedelta(days=int(days))).isoformat().replace("+00:00", "Z") all_video_ids = [] video_meta = {} # videoId -> {keyword, title, channelTitle, publishedAt} for kw in keywords: st.write(f"ðŸ”Ž Searching: **{kw}**") search_params = { "part": "snippet", "q": kw, "type": "video", "order": "viewCount", "publishedAfter": start_date, "maxResults": int(max_per_keyword), "key": API_KEY, } # duration filter (optional) if duration_filter != "any": search_params["videoDuration"] = duration_filter # "medium" => 4-20 min (shorts mostly gone) data, err = yt_get(YOUTUBE_SEARCH_URL, search_params) if err: st.error(f"Search error for '{kw}': {err}") continue items = (data or {}).get("items", []) if not items: st.info(f"No videos found for: {kw}") continue for it in items: vid = (it.get("id") or {}).get("videoId") sn = it.get("snippet") or {} if not vid: continue # store first-seen meta (avoid overwriting) if vid not in video_meta: video_meta[vid] = { "Keyword": kw, "Title": sn.get("title", ""), "Channel": sn.get("channelTitle", ""), "PublishedAt": sn.get("publishedAt", ""), "URL": f"https://www.youtube.com/watch?v={vid}", } all_video_ids.append(vid) if not all_video_ids: st.warning("No videos collected. Try increasing days or turning off Exclude Shorts.") st.stop() # Fetch video stats in batches rows = [] now = datetime.now(timezone.utc) for ids in chunk(all_video_ids, 50): stats_params = {"part": "statistics", "id": ",".join(ids), "key": API_KEY} stats_data, err = yt_get(YOUTUBE_VIDEO_URL, stats_params) if err: st.error(f"Stats error: {err}") continue for v in (stats_data or {}).get("items", []): vid = v.get("id") stats = (v.get("statistics") or {}) views = int(stats.get("viewCount", 0) or 0) meta = video_meta.get(vid, {}) published_str = meta.get("PublishedAt", "") if not published_str: continue published = parse_yt_time(published_str) age_days = max((now - published).total_seconds() / 86400.0, 0.01) # avoid divide by zero views_per_day = views / age_days rows.append({ "Keyword": meta.get("Keyword", ""), "Title": meta.get("Title", ""), "Channel": meta.get("Channel", ""), "PublishedAt": published_str, "Views": views, "Views/Day": round(views_per_day, 2), "URL": meta.get("URL", ""), }) if not rows: st.warning("No stats rows. Try again.") st.stop() df = pd.DataFrame(rows).sort_values("Views/Day", ascending=False) st.success(f"Found {len(df)} videos. Sorted by Views/Day (viral speed).") st.dataframe( df, use_container_width=True, column_config={"URL": st.column_config.LinkColumn("URL")} ) csv = df.to_csv(index=False).encode("utf-8") st.download_button("Download CSV", data=csv, file_name="viral_videos_last_days.csv", mime="text/csv")
